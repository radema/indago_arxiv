{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:42:50.531175Z",
     "start_time": "2020-01-03T11:42:47.509583Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import qgrid\n",
    "from functools import reduce\n",
    "#import collections\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:42:50.621465Z",
     "start_time": "2020-01-03T11:42:50.571318Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
    "stopWordList=stopwords.words('english')\n",
    "stopWordList.remove('no')\n",
    "stopWordList.remove('not')\n",
    "lemma=WordNetLemmatizer()\n",
    "token=ToktokTokenizer()\n",
    "\"\"\" In apparenza questa funzione non serve a niente\n",
    "def lemitizeWords(text):\n",
    "    words=token.tokenize(text)\n",
    "    listLemma=[]\n",
    "    for w in words:\n",
    "        x=lemma.lemmatize(w,'v')\n",
    "        listLemma.append(x)\n",
    "    return text\n",
    "\"\"\"\n",
    "def stopWordsRemove(text):\n",
    "    wordList=[x.lower().strip() for x in token.tokenize(text)]\n",
    "    removedList=[x for x in wordList if not (x in stopWordList)]\n",
    "    text=' '.join(removedList)\n",
    "    return text\n",
    "def stemWords(text):\n",
    "    wordList=[x.lower().strip() for x in token.tokenize(text)]\n",
    "    stemmedlist = map(lambda x: stemmer.stem(x), wordList)\n",
    "    text = ' '.join(stemmedlist)\n",
    "    return text\n",
    "def removeAscendingChar(data):\n",
    "    data=unicodedata.normalize('NFKD',data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return data\n",
    "def removeCharDigit(text):\n",
    "    stringa='`~@#$%&*()[!{;”:\\’><.,/?”}]'\n",
    "    text = ''.join(list(map(lambda w: ' ' if w in stringa else w, text)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:42:50.655623Z",
     "start_time": "2020-01-03T11:42:50.648283Z"
    }
   },
   "outputs": [],
   "source": [
    "source_path = 'D:/Users/rdemaio/Desktop/Practice ML/indago_arxiv/data/source/'\n",
    "end_path = 'D:/Users/rdemaio/Desktop/Practice ML/indago_arxiv/data/processed/'\n",
    "filename = 'sourcesample_v2017.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:42:55.178540Z",
     "start_time": "2020-01-03T11:42:50.692573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\rdemaio\\AppData\\Local\\Continuum\\anaconda3\\envs\\arxiv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "cols = ['Unnamed: 0', 'abstract', 'authors','categories','created','updated','id','title']\n",
    "df = pd.read_csv(source_path + filename)[cols]\n",
    "df.columns= ['index', 'abstract', 'authors','categories','created','updated','id','title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:43:03.163031Z",
     "start_time": "2020-01-03T11:42:55.254583Z"
    }
   },
   "outputs": [],
   "source": [
    "sa = df\n",
    "\n",
    "#parse dates\n",
    "create_date = sa.created.str.replace('-','').map(lambda x: float(x))\n",
    "update_date = sa.updated.str.replace('-','').map(lambda x: float(x))\n",
    "\n",
    "\"\"\"\n",
    "combine create and update dates into last_update\n",
    "last_update is the last update date or the creation date if there's none\n",
    "\"\"\"\n",
    "last_update = update_date\n",
    "for index, value in last_update.iteritems():\n",
    "    if np.isnan(value):\n",
    "        last_update[index] = create_date[index]\n",
    "sa['last_update'] = last_update\n",
    "\n",
    "#drop columns we don't need anymore\n",
    "sa = sa.drop(columns = ['created','updated'])\n",
    "\n",
    "#drop duplicates and get the last update per paper (id)\n",
    "# sort by id, updated\n",
    "sa = sa.sort_values(by = ['title','last_update'], ascending = [True, False])\n",
    "#drop duplicates (1 line per paper)\n",
    "sa_1 = sa.drop_duplicates(['title'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:43:52.077106Z",
     "start_time": "2020-01-03T11:43:03.303425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\rdemaio\\AppData\\Local\\Continuum\\anaconda3\\envs\\arxiv\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Users\\rdemaio\\AppData\\Local\\Continuum\\anaconda3\\envs\\arxiv\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "From sa_1 restrict to the necessary columns\n",
    "Expand the dataset w.r.t. categories and authors\n",
    "\"\"\"\n",
    "sa_2 = sa_1[['id','last_update','index','abstract']]\n",
    "sa_2['title'] = sa_1.title.replace('  ',' ',regex = True)\n",
    "sa_2['categories'] = sa_1.categories.str.split()\n",
    "sa_2['authors'] =  sa_1.authors.apply(lambda x: list(map(str.strip, x.strip('[]').replace(\"'\",'').replace(\"-\",\"\").split(','))))\n",
    "sa_2['authors'] = sa_2['authors'].apply(lambda x: list(map(removeAscendingChar, x)))\n",
    "sa_2['title'] = sa_2.title.apply(lambda x: removeAscendingChar(x))\n",
    "sa_2['abstract'] = sa_2.abstract.apply(lambda x: removeAscendingChar(x))\n",
    "\n",
    "authors = (sa_2['authors'].apply(lambda x: pd.Series(list(x)))\n",
    "                  .stack()\n",
    "                  .rename('author')\n",
    "                  .reset_index(level=1, drop=True))\n",
    "#print(authors)\n",
    "category = (sa_2['categories'].apply(lambda x: pd.Series(list(x)))\n",
    "                  .stack()\n",
    "                  .rename('category')\n",
    "                  .reset_index(level=1, drop=True))\n",
    "\n",
    "sa_3 = sa_2.join(authors).join(category).reset_index(drop=True)\n",
    "sa_3 = sa_3.drop(columns = ['categories','authors'])\n",
    "\n",
    "#sa_3['author'] = sa_3.author.apply(lambda x: removeAscendingChar(x))\n",
    "#sa_3['title'] = sa_3.title.apply(lambda x: removeAscendingChar(x))\n",
    "#sa_3['abstract'] = sa_3.abstract.apply(lambda x: removeAscendingChar(x))\n",
    "sa_3['text'] = sa_3['title'] + ' ' + sa_3['abstract'] + ' '\n",
    "sa_3['macro'] = sa_3.category.apply(lambda x: x.split('.')[0])\n",
    "\n",
    "sa_3 = sa_3.dropna(subset = ['author','title','abstract','category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T11:32:56.724080Z",
     "start_time": "2020-01-03T11:32:26.545995Z"
    }
   },
   "outputs": [],
   "source": [
    "f_transaction = sa_3[['id','author','category','macro', 'last_update']]\n",
    "f_transaction.to_csv(end_path+'f_transaction.csv')\n",
    "sa_2.to_csv(end_path + 's_pap_to_ingest_neo4j.csv')\n",
    "s_papers = sa_3\n",
    "s_papers.to_csv(end_path+'f_papers.csv',columns = s_papers)\n",
    "s_papers_cat = sa_3[['macro','category','text','id']].drop_duplicates(['macro','category','text'],keep = 'first')\n",
    "s_papers_cat.to_csv(end_path+'f_papers_cat.csv',columns = s_papers_cat.columns)\n",
    "#s_papers_macro = sa_3[['macro','text','id']].drop_duplicates(['macro','text'],keep = 'first')\n",
    "#s_papers_macro.to_csv(end_path+'f_papers_macro.csv',columns = s_papers_macro.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T13:13:47.225997Z",
     "start_time": "2019-12-29T13:10:34.712514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#corpus = reduce(lambda x,y: x + y, s_papers.text)\n",
    "corpus_cat = s_papers_cat.groupby(['macro','category']).sum()\n",
    "corpus_cat['cleaned_text'] = corpus_cat.text.apply(lambda x: removeCharDigit(x))\n",
    "corpus_cat['cleaned_text'] = corpus_cat.cleaned_text.apply(lambda x: stemWords(stopWordsRemove(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T13:21:26.964508Z",
     "start_time": "2019-12-29T13:21:26.097855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_macro = corpus_cat[['cleaned_text','text']].groupby(['macro']).sum()\n",
    "corpus = corpus_macro.sum()\n",
    "corpus_cat.reset_index(inplace=True)\n",
    "corpus_macro.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T13:23:15.951652Z",
     "start_time": "2019-12-29T13:23:15.943646Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Store corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T13:26:12.399214Z",
     "start_time": "2019-12-29T13:26:05.167968Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#corpus_macro.reset_index(inplace=True)\n",
    "corpus_cat.to_csv(end_path + 'corpus_cat.csv', columns= corpus_cat.columns)\n",
    "corpus_macro.to_csv(end_path + 'corpus_macro.csv',columns=corpus_macro.columns)\n",
    "corpus.to_csv(end_path + 'corpus.csv',header = True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
